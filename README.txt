This project consists of two parts:

1. Identifying and accurately translating eye-coordinates (X-eye,Y-eye,Z-eye) to a point on a screen (X-screen,Y-screen), this allows the user
to control the mouse pointer using their gaze.




2. Identifying and mapping hand motions to mouse functionality.


A few questions need to be answered: do we follow the data based approach using statistical models on our data to determine (classify) pixel-coordinates of our mouse, or

assumption based approach: using hand-crafted algorithm based on certain assumptions?

